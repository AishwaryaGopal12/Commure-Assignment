"""
Natural language to SQL conversion using LangGraph.

This module contains the main LangChain orchestrator class that coordinates
the pipeline of converting user questions into validated SQL queries using
a graph-based agent workflow with critic review.
"""
import re
import sqlalchemy
from langchain_core.messages import HumanMessage
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from tools import update_delete_drop_insert, validate_sql, check_error, get_schema
from lang_graph import Agent


class LangChain:
    """
    Main orchestrator class for natural language to SQL conversion using LangGraph.

    This class coordinates the entire pipeline of converting user questions into
    validated SQL queries. It leverages multiple agents and tools to ensure the
    generated SQL is syntactically correct, functionally sound, and semantically
    aligned with the user's intent.

    Architecture:
        1. Uses LangGraph Agent for iterative SQL generation
        2. Employs structured output parsing via Pydantic models
        3. Integrates multiple validation tools (syntax, security, semantics)
        4. Implements critic review pattern for quality assurance
        5. Supports multiple SQL extraction fallback strategies

    Attributes:
        model: The language model (e.g., ChatOpenAI) used for SQL generation.
        parser (PydanticOutputParser): Parser for structured SQL response extraction.
        format_instructions (str): JSON schema instructions for LLM output formatting.
        prompt (str): System prompt template with detailed instructions.
        engine: SQLAlchemy engine for database connection and query validation.
        tools (list): List of tool functions available to the agent.
    """

    def __init__(self, model, sqlite_url):
        """
        Initialize the LangChain SQL generation system.

        Sets up all necessary components including the parser, prompt template,
        database connection, and available tools for the agent.

        Args:
            model: Language model instance (e.g., ChatOpenAI) supporting tool calling.
            sqlite_url (str): SQLAlchemy connection string for SQLite database.
                             Format: "sqlite:///path/to/database.db"

        Note:
            The prompt template includes detailed instructions about:
            - SQL generation rules (SELECT only, no mutations)
            - Schema understanding requirements
            - Case-insensitive comparison guidelines
            - Tie-breaking logic for TOP-N queries
            - Critic agent integration
        """
        # Store the language model for later use in SQL generation
        self.model = model

        # Define Pydantic model for structured output from the LLM
        # This ensures consistent, parseable responses
        class SQLResponse(BaseModel):
            """
            Structured response format for SQL generation.

            Attributes:
                sql (str): The generated SQL query.
                scratchpad (str): Step-by-step reasoning explaining how the SQL was derived.
                tables_used (str): Comma-separated list of tables referenced in the query.
            """
            sql: str = Field(...,
                             description="SQL statement generated by the LLM")
            scratchpad: str = Field(...,
                                    description="Step-by-step reasoning for generating SQL")
            tables_used: str = Field(...,
                                     description="Comma-separated table names used")

        # Initialize parser with the SQLResponse model
        # This parser will extract structured data from LLM responses
        self.parser = PydanticOutputParser(pydantic_object=SQLResponse)

        # Get formatting instructions that will be injected into the prompt
        # This tells the LLM exactly what JSON structure to output
        self.format_instructions = self.parser.get_format_instructions()

        # Define the comprehensive system prompt template
        # This prompt is critical - it guides the entire SQL generation process
        self.prompt = """
            You are an AI assistant that converts natural language questions into SQL queries.
            To do this, you will be provided with three key pieces of information:

            The actual natural language question to convert into an SQL query:
            <question>
            {QUESTION}
            </question>

            Follow the instructions below:
            1. Your task is to generate an SQL query that will retrieve the data needed to answer the question, based on the database schema.
            2. First, carefully study the provided schema  to understand the structure of the database and how the examples map natural language to SQL for this schema.
            3. You can only do select queries. Incase of any other queries, you should return a SQL statement that prints "only select query allowed" in the <sql> tag in step 4. Return only the required columns to answer the question. Do not return extra columns
            4. You should validate the SQL you have created for syntax and function errors. If there are any errors, you should go back and address the error in the SQL. Recreate the SQL based by addressing the error.
            5. String comparisons must match actual values in the database. If unsure of exact capitalization, use case-insensitive comparisons, e.g., WHERE LOWER(column) = LOWER('Available'). Never guess casing.
            6. When a query asks for an extreme (max/min/top-1), if less than 5 rows are tied, return all tied rows. Otherwise, apply a deterministic tie-breaker: ORDER BY metric DESC, patient_id ASC and document the tie.
            7. There is also a critic agent that will review the SQL and provide clear, actionable feedback. If the critic finds any issues, you should address the issue and return the corrected SQL.
            7. Follow the rules and return output in the exact JSON structure below.
            Formatting requirements (MANDATORY):
            {format_instructions}
        """

        # Create SQLAlchemy engine for database connection
        # This engine is used for both validation and execution
        self.engine = sqlalchemy.create_engine(
            url=sqlite_url,
            pool_pre_ping=True,  # Verify connections before using them
            pool_recycle=300,    # Recycle connections after 5 minutes
            future=True          # Use SQLAlchemy 2.0 style
        )

        # Define the tools available to the agent
        # These tools enable validation, error checking, and schema introspection
        self.tools = [
            update_delete_drop_insert,  # Ensures query is SELECT only
            check_error,                # Provides guidance on SQL errors
            get_schema,                 # Returns database schema information
            validate_sql                # Validates syntax and functionality
        ]

    def get_sql(self, question):
        """
        Convert a natural language question into a validated SQL query.

        This is the main entry point for SQL generation. It orchestrates:
        1. Initial SQL generation via LangGraph agent
        2. Multiple extraction strategies to parse SQL from LLM response
        3. Critic review for semantic validation
        4. Optional regeneration based on critic feedback

        Args:
            question (str): Natural language question to convert to SQL.

        Returns:
            tuple: A 2-tuple containing:
                - sql_query (str): The final validated SQL query.
                - metadata (dict): Token usage statistics from the LLM call.

        Raises:
            ValueError: If SQL cannot be extracted from LLM response or if
                       the agent returns invalid output.

        Process Flow:
            1. Format prompt with question and instructions
            2. Create LangGraph agent with tools
            3. Invoke agent to generate initial SQL
            4. Extract SQL using multiple fallback strategies
            5. Submit SQL to critic for review
            6. If critique fails, regenerate SQL with feedback
            7. Return final SQL and metadata
        """
        # Format the system prompt by injecting the question and format instructions
        prompt = self.prompt.format(
            QUESTION=question,
            format_instructions=self.format_instructions
        )

        # Create a new agent instance with the formatted prompt
        # Each question gets a fresh agent to avoid state contamination
        abot = Agent(self.model, self.tools, system=prompt)

        # Create a thread context for this conversation
        # This allows tracking the conversation state across multiple turns
        thread = {"configurable": {"thread_id": "1"}}

        # Invoke the agent with the initial question
        # The agent will iteratively call tools until it generates a final response
        response = abot.graph.invoke(
            {"messages": [
                HumanMessage(content=[{"type": "text", "text": question}])
            ],
             "thread": thread}
        )

        # Check if the response indicates a non-SELECT query restriction
        if response.get("messages")[-1].content == "Can only do select queries":
            return None

        # Print the raw LLM response for debugging/logging
        print(response.get("messages")[-1].content)

        # Extract content from the last message in the response
        content = response.get("messages")[
            -1].content if response and response.get("messages") else None

        # Validate that we received a string response
        if not isinstance(content, str):
            raise ValueError(
                "LLM response content is missing or not a string; cannot extract SQL.")

        # --- SQL Extraction Strategy ---
        # Try multiple methods to extract SQL from the LLM response
        # This resilience is important because LLMs may format output inconsistently

        sql_query = None

        # Strategy 1: Try strict JSON parsing using PydanticOutputParser
        try:
            parsed = self.parser.parse(content)
            sql_query = (parsed.sql or "").strip()
        except Exception:
            # If JSON parsing fails, continue to fallback strategies
            pass

        # Strategy 2: Look for <sql>...</sql> tags (case-insensitive)
        if not sql_query:
            # Use regex to find SQL enclosed in XML-style tags
            msg = re.search(r"<\s*sql\s*>([\s\S]*?)<\s*/\s*sql\s*>", content,
                          re.IGNORECASE)
            if msg:
                sql_query = msg.group(1).strip()

        # Strategy 3: Look for SQL code blocks (```sql...``` or ```...```)
        if not sql_query:
            # Search for markdown-style code blocks
            msg = re.search(r"```(?:sql|SQL)?\s*([\s\S]*?)```", content)
            if msg:
                sql_query = msg.group(1).strip()

        # Strategy 4: Heuristic search for SELECT/WITH statements
        # Look for SQL keywords followed by content up to semicolon
        if not sql_query:
            msg = re.search(r"(?is)\b(SELECT|WITH)\b[\s\S]*?;", content)
            if msg:
                sql_query = msg.group(0).strip()

        # If all extraction strategies failed, raise an error
        if not sql_query:
            raise ValueError(
                "Failed to extract SQL from LLM response. Ensure the model outputs valid JSON or encloses SQL in <sql>...</sql>.")

        # --- Critic Review Stage ---
        # Submit the generated SQL to a critic agent for semantic validation
        critic_decision = run_critic_review(self.model, question, sql_query)

        # If critic doesn't approve, regenerate SQL with feedback
        if "approve" not in critic_decision.lower():
            # Construct feedback message for the agent
            feedback_msg = f"The SQL you generated does not fully answer the question. Feedback: {critic_decision}"
            print("[Critic Feedback]:", feedback_msg)

            # Invoke agent again with the original response + critic feedback
            # This gives the agent a chance to fix issues identified by the critic
            followup = abot.graph.invoke({
                "messages": response.get("messages") + [
                    HumanMessage(content=feedback_msg)],
                "thread": thread
            })

            # Re-extract SQL from the regenerated response
            content = followup.get("messages")[-1].content if followup.get(
                "messages") else ""

            # Try JSON parsing first
            try:
                parsed = self.parser.parse(content)
                sql_query = (parsed.sql or "").strip()
            except Exception:
                # Fall back to XML tag extraction
                sql_query = None
                msg = re.search(r"<\s*sql\s*>([\s\S]*?)<\s*/\s*sql\s*>", content,
                              re.IGNORECASE)
                if msg:
                    sql_query = msg.group(1).strip()

        # Return the final SQL query and token usage metadata
        return sql_query, response.get("messages")[-1].response_metadata.get(
            "token_usage", {})

    def run_query(self, sql_query):
        """
        Execute a validated SQL query against the database.

        This method runs the provided SQL query with foreign key constraints
        enabled and returns all results.

        Args:
            sql_query (str): The SQL query to execute (should be SELECT only).

        Returns:
            list: List of result tuples from the query execution.
                 Each tuple represents one row of results.

        Raises:
            sqlalchemy.exc.SQLAlchemyError: If the query execution fails.

        Note:
            - Foreign key constraints are explicitly enabled via PRAGMA
            - Connection is automatically closed after query execution
            - For large result sets, consider using pagination
        """
        # Use context manager to ensure connection is properly closed
        with self.engine.connect() as conn:
            # Enable foreign key constraints for data integrity
            # SQLite has FK constraints off by default
            conn.exec_driver_sql("PRAGMA foreign_keys = ON;")

            # Execute the SQL query using SQLAlchemy's text() for safety
            result = conn.execute(sqlalchemy.text(sql_query))

            # Fetch and return all results as a list of tuples
            return result.fetchall()


def run_critic_review(model, question: str, sql_string: str) -> str:
    """
    Use a separate LLM instance to critique and review generated SQL.

    This function implements a "critic agent" pattern where a second LLM reviews
    the work of the first. The critic checks if the SQL correctly answers the
    user's question and provides actionable feedback if issues are found.

    Args:
        model: Language model instance to use for the critique.
        question (str): The original natural language question from the user.
        sql_string (str): The SQL query that was generated to answer the question.

    Returns:
        str: Either "approve" if the SQL is correct, or a detailed critique
             describing what's wrong or missing. The feedback is stripped of
             leading/trailing whitespace.

    Note:
        This function does NOT return corrected SQL - it only provides feedback.
        The original agent should use this feedback to regenerate the SQL if needed.
    """
    # Construct a detailed prompt that asks the LLM to act as a critic
    # The prompt clearly defines the critic's role and expected output format
    prompt = f"""You are a SQL critic for a hospital database.
              Your task is to review the SQL and provide clear,
              actionable feedback. DO NOT write the corrected SQL.
              Also, keep in mind that the agent is only allowed to run SELECT queries.
              Anything that has got to do with UPDATE/INSERT/DELETE/DROP is not allowed.
              Do not provide feedback in that direction.

            Question:
            {question}

            SQL:
            {sql_string}

            What to do:
            - If the SQL answers the question correctly, just respond with: approve
            - Otherwise, describe what's wrong or missing in one short paragraph. Be specific but concise.

            Your feedback:"""

    # Invoke the critic model with the constructed prompt
    # We use HumanMessage to simulate a user asking for a review
    resp = model.invoke([HumanMessage(content=prompt)])

    # Extract the text content from the response and clean whitespace
    # If content is None, use empty string as fallback
    return (resp.content or "").strip()